# %%
import os
import sys
import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    roc_curve,
    auc,
)
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# %%
# --- Cáº¤U HÃŒNH ---
# Option 1: Äá»c tá»« file Ä‘Ã£ gá»™p sáºµn (náº¿u Ä‘Ã£ cháº¡y combine_datasets.py)
COMBINED_DATASET_PATH = os.path.join("data", "final_dataset_shuffled.csv")

# Option 2: Äá»c tá»« 2 file riÃªng biá»‡t
ATTACK_PATH = os.path.join("data", "attack_hping3.csv")
NORMAL_PATH = os.path.join("data", "normal_web.csv")

# ThÆ° má»¥c lÆ°u model
MODELS_DIR = "models"

# Chá»n mode: "combined" (Ä‘á»c tá»« file Ä‘Ã£ gá»™p) hoáº·c "separate" (Ä‘á»c tá»« 2 file riÃªng)
LOAD_MODE = "combined"  # Thay Ä‘á»•i thÃ nh "separate" náº¿u muá»‘n Ä‘á»c tá»« 2 file riÃªng

# 12 Feature chuáº©n cá»§a Lite Model (Khá»›p vá»›i Sniffer vÃ  Detector)
SELECTED_FEATURES = [
    "Flow Duration", 
    "Total Fwd Packets", 
    "Total Backward Packets",
    "Total Length of Fwd Packets", 
    "Total Length of Bwd Packets",
    "Fwd Packet Length Max", 
    "Fwd Packet Length Min", 
    "Fwd Packet Length Mean",
    "Flow IAT Mean", 
    "Fwd IAT Mean", 
    "Fwd Header Length",
    "Flow IAT Std"
]

# %%
def load_and_process_data():
    print("="*50)
    print("BÆ¯á»šC 1: Táº¢I VÃ€ GÃN NHÃƒN Dá»® LIá»†U")
    print("="*50)

    # Kiá»ƒm tra mode load
    if LOAD_MODE == "combined":
        # Äá»c tá»« file Ä‘Ã£ gá»™p sáºµn
        if not os.path.exists(COMBINED_DATASET_PATH):
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file Ä‘Ã£ gá»™p: {COMBINED_DATASET_PATH}")
            print(f"ğŸ’¡ Chuyá»ƒn sang mode 'separate' Ä‘á»ƒ Ä‘á»c tá»« 2 file riÃªng...")
            # Fallback to separate mode
            return load_from_separate_files()
        
        print(f"ğŸ“– Äang Ä‘á»c tá»« file Ä‘Ã£ gá»™p: {COMBINED_DATASET_PATH}")
        try:
            df_merged = pd.read_csv(COMBINED_DATASET_PATH)
        except Exception as e:
            print(f"âš ï¸ Lá»—i Ä‘á»c CSV: {e}")
            return None, None, None
        
        # Kiá»ƒm tra cÃ³ cá»™t Label khÃ´ng
        if 'Label' not in df_merged.columns:
            print("âš ï¸ File khÃ´ng cÃ³ cá»™t 'Label'. Chuyá»ƒn sang mode 'separate'...")
            return load_from_separate_files()
        
        print(f"ğŸ“Š Dá»¯ liá»‡u tá»« file Ä‘Ã£ gá»™p: {len(df_merged)} dÃ²ng")
        print(f"   - Normal (Label=0): {(df_merged['Label'] == 0).sum()} dÃ²ng")
        print(f"   - Attack (Label=1): {(df_merged['Label'] == 1).sum()} dÃ²ng")
        
        # Lá»c Feature (Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t cáº§n thiáº¿t)
        for col in SELECTED_FEATURES:
            if col not in df_merged.columns:
                df_merged[col] = 0
        
        df_merged = df_merged[SELECTED_FEATURES + ['Label']]
        
        # LÃ m sáº¡ch dá»¯ liá»‡u
        df_merged.replace([np.inf, -np.inf], np.nan, inplace=True)
        df_merged.fillna(0, inplace=True)
        
        # CÃ¢n báº±ng dá»¯ liá»‡u (náº¿u cáº§n)
        normal_count = (df_merged['Label'] == 0).sum()
        attack_count = (df_merged['Label'] == 1).sum()
        
        if normal_count != attack_count:
            print(f"âš–ï¸ Äang cÃ¢n báº±ng dá»¯ liá»‡u (Normal: {normal_count}, Attack: {attack_count})...")
            if attack_count < normal_count:
                df_attack = df_merged[df_merged['Label'] == 1]
                df_normal = df_merged[df_merged['Label'] == 0]
                df_attack_balanced = resample(df_attack, replace=True, n_samples=normal_count, random_state=42)
                df_merged = pd.concat([df_attack_balanced, df_normal])
            else:
                df_attack = df_merged[df_merged['Label'] == 1]
                df_normal = df_merged[df_merged['Label'] == 0]
                df_normal_balanced = resample(df_normal, replace=True, n_samples=attack_count, random_state=42)
                df_merged = pd.concat([df_attack, df_normal_balanced])
            
            # Shuffle láº¡i sau khi cÃ¢n báº±ng
            df_merged = df_merged.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"âœ… Tá»•ng dá»¯ liá»‡u sau khi xá»­ lÃ½: {len(df_merged)} dÃ²ng.")
        
    else:
        # Äá»c tá»« 2 file riÃªng biá»‡t (mode cÅ©)
        return load_from_separate_files()
    
    X = df_merged[SELECTED_FEATURES].values
    y = df_merged['Label'].values.astype(int)
    
    return X, y, df_merged

# %%
def load_from_separate_files():
    """Load data tá»« 2 file riÃªng biá»‡t (mode cÅ©)"""
    if not os.path.exists(ATTACK_PATH) or not os.path.exists(NORMAL_PATH):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u. HÃ£y cháº¯c cháº¯n báº¡n Ä‘Ã£ cÃ³ '{ATTACK_PATH}' vÃ  '{NORMAL_PATH}'.")

    # Äá»c file
    try:
        df_attack = pd.read_csv(ATTACK_PATH)
        df_normal = pd.read_csv(NORMAL_PATH)
    except Exception as e:
        print(f"âš ï¸ Lá»—i Ä‘á»c CSV: {e}")
        return None, None, None

    # GÃ¡n nhÃ£n (Labeling)
    df_attack['Label'] = 1  # 1 = DDoS
    df_normal['Label'] = 0  # 0 = Normal

    print(f"ğŸ“Š Dá»¯ liá»‡u gá»‘c: Attack={len(df_attack)} dÃ²ng | Normal={len(df_normal)} dÃ²ng")

    # Lá»c Feature (Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t cáº§n thiáº¿t Ä‘á»ƒ trÃ¡nh lá»—i thiáº¿u cá»™t)
    # Náº¿u file thiáº¿u cá»™t nÃ o Ä‘Ã³, ta Ä‘iá»n 0 vÃ o
    for col in SELECTED_FEATURES:
        if col not in df_attack.columns:
            df_attack[col] = 0
        if col not in df_normal.columns:
            df_normal[col] = 0
            
    df_attack = df_attack[SELECTED_FEATURES + ['Label']]
    df_normal = df_normal[SELECTED_FEATURES + ['Label']]

    # CÃ¢n báº±ng dá»¯ liá»‡u (Balancing)
    # Upsample nhÃ³m Ã­t hÆ¡n Ä‘á»ƒ cÃ¢n báº±ng 1:1
    if len(df_attack) < len(df_normal):
        print("âš–ï¸ Äang nhÃ¢n báº£n dá»¯ liá»‡u Attack Ä‘á»ƒ cÃ¢n báº±ng...")
        df_attack_balanced = resample(df_attack, replace=True, n_samples=len(df_normal), random_state=42)
        df_merged = pd.concat([df_attack_balanced, df_normal])
    else:
        print("âš–ï¸ Äang nhÃ¢n báº£n dá»¯ liá»‡u Normal Ä‘á»ƒ cÃ¢n báº±ng...")
        df_normal_balanced = resample(df_normal, replace=True, n_samples=len(df_attack), random_state=42)
        df_merged = pd.concat([df_attack, df_normal_balanced])

    # XÃ¡o trá»™n dá»¯ liá»‡u
    df_merged = df_merged.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"âœ… Tá»•ng dá»¯ liá»‡u sau khi cÃ¢n báº±ng: {len(df_merged)} dÃ²ng.")
    
    X = df_merged[SELECTED_FEATURES].values
    y = df_merged['Label'].values.astype(int)
    
    return X, y, df_merged

# %%
def explore_data(df_features: pd.DataFrame, labels: np.ndarray):
    """Data Exploration: Plot feature distributions and statistics"""
    print("\n" + "="*50)
    print("BÆ¯á»šC 1.5: DATA EXPLORATION")
    print("="*50)
    
    os.makedirs(MODELS_DIR, exist_ok=True)
    
    # Plot distribution of each feature
    n_features = len(df_features.columns)
    n_cols = 4
    n_rows = (n_features + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))
    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else []
    
    for idx, col in enumerate(df_features.columns):
        ax = axes[idx] if idx < len(axes) else None
        if ax is None:
            continue
        
        # Plot distribution for each class
        normal_data = df_features[labels == 0][col].dropna()
        attack_data = df_features[labels == 1][col].dropna()
        
        ax.hist(normal_data, bins=50, alpha=0.5, label="Normal", density=True, color="green")
        ax.hist(attack_data, bins=50, alpha=0.5, label="Attack", density=True, color="red")
        ax.set_title(f"{col}")
        ax.set_xlabel("Value")
        ax.set_ylabel("Density")
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # Hide unused subplots
    for idx in range(n_features, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(os.path.join(MODELS_DIR, "feature_distributions.png"), dpi=300, bbox_inches="tight")
    plt.close()
    print("ğŸ“Š ÄÃ£ lÆ°u feature_distributions.png")
    
    # Print statistics
    print("\nğŸ“ˆ Feature Statistics:")
    print(df_features.describe())
    
    # Class distribution
    print(f"\nğŸ“Š Class Distribution:")
    print(f"Normal: {(labels == 0).sum()} ({(labels == 0).mean() * 100:.2f}%)")
    print(f"Attack: {(labels == 1).sum()} ({(labels == 1).mean() * 100:.2f}%)")

# %%
def build_cnn_model(input_shape):
    # Kiáº¿n trÃºc CNN Lite giá»‘ng train_lite_model.py cÅ©
    inputs = keras.Input(shape=(input_shape, 1))
    x = layers.Conv1D(filters=32, kernel_size=3, activation="relu", padding="same")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(pool_size=2)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(rate=0.2)(x)  # ThÃªm Dropout Ä‘á»ƒ chá»‘ng overfitting
    outputs = layers.Dense(1, activation="sigmoid")(x)
    
    model = keras.Model(inputs=inputs, outputs=outputs, name="cnn_lite_v2")
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# %%
def main():
    # Load Data
    X, y, df_merged = load_and_process_data()
    if X is None:
        return

    # Data Exploration
    df_features = df_merged[SELECTED_FEATURES].copy()
    explore_data(df_features, y)

    print("\n" + "="*50)
    print("BÆ¯á»šC 2: CHUáº¨N Bá»Š TRAIN")
    print("="*50)

    # Split Train/Test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale Data (Quan trá»ng)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Reshape cho CNN (Samples, Features, 1)
    X_train_reshaped = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)
    X_test_reshaped = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)

    print(f"Data Shape: {X_train_reshaped.shape}")

    # Train
    print("\n" + "="*50)
    print("BÆ¯á»šC 3: TRAINING MODEL")
    print("="*50)
    
    model = build_cnn_model(X_train_reshaped.shape[1])
    history = model.fit(
        X_train_reshaped, y_train,
        epochs=15, 
        batch_size=32, 
        validation_data=(X_test_reshaped, y_test),
        verbose=1
    )

    # Evaluate
    print("\n" + "="*50)
    print("BÆ¯á»šC 4: MODEL EVALUATION")
    print("="*50)
    
    y_test_proba = model.predict(X_test_reshaped, verbose=0).flatten()
    y_test_pred = (y_test_proba > 0.5).astype(int)
    
    acc = accuracy_score(y_test, y_test_pred)
    prec = precision_score(y_test, y_test_pred)
    rec = recall_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred)
    cm = confusion_matrix(y_test, y_test_pred)
    fpr, tpr, _ = roc_curve(y_test, y_test_proba)
    roc_auc = auc(fpr, tpr)
    
    print(f"ğŸ“Š Metrics trÃªn Test Set:")
    print(f"  Accuracy:  {acc*100:.2f}%")
    print(f"  Precision: {prec*100:.2f}%")
    print(f"  Recall:    {rec*100:.2f}%")
    print(f"  F1-Score:   {f1*100:.2f}%")
    print(f"  ROC AUC:    {roc_auc:.4f}")

    # Save Artifacts
    print("\n" + "="*50)
    print("BÆ¯á»šC 5: LÆ¯U MODEL")
    print("="*50)
    
    if not os.path.exists(MODELS_DIR):
        os.makedirs(MODELS_DIR)

    model_path = os.path.join(MODELS_DIR, "cnn_lite_model.h5")
    scaler_path = os.path.join(MODELS_DIR, "cnn_lite_scaler.pkl")
    features_path = os.path.join(MODELS_DIR, "cnn_lite_feature_names.pkl")

    model.save(model_path)
    joblib.dump(scaler, scaler_path)
    joblib.dump(SELECTED_FEATURES, features_path)

    print(f"ğŸ’¾ ÄÃ£ lÆ°u Model táº¡i: {model_path}")
    print(f"ğŸ’¾ ÄÃ£ lÆ°u Scaler táº¡i: {scaler_path}")
    print(f"ğŸ’¾ ÄÃ£ lÆ°u Feature names táº¡i: {features_path}")

    # Save plots
    print("\n" + "="*50)
    print("BÆ¯á»šC 6: LÆ¯U PLOTS")
    print("="*50)
    
    # Training History
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["accuracy"], label="train_acc")
    plt.plot(history.history["val_accuracy"], label="val_acc")
    plt.title("Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history["loss"], label="train_loss")
    plt.plot(history.history["val_loss"], label="val_loss")
    plt.title("Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(MODELS_DIR, "training_history.png"), dpi=300, bbox_inches="tight")
    plt.close()
    print("ğŸ“Š ÄÃ£ lÆ°u training_history.png")
    
    # ROC Curve
    plt.figure(figsize=(5, 4))
    plt.plot(fpr, tpr, label=f"AUC={roc_auc:.4f}")
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve (test)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(MODELS_DIR, "roc_curve.png"), dpi=300, bbox_inches="tight")
    plt.close()
    print("ğŸ“Š ÄÃ£ lÆ°u roc_curve.png")
    
    # Confusion Matrix
    plt.figure(figsize=(5, 4))
    plt.imshow(cm, cmap="Blues")
    plt.title("Confusion Matrix (test)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.xticks([0, 1], ["Normal", "Attack"])
    plt.yticks([0, 1], ["Normal", "Attack"])
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, str(cm[i, j]), ha="center", va="center", color="black", fontsize=14)
    plt.tight_layout()
    plt.savefig(os.path.join(MODELS_DIR, "confusion_matrix.png"), dpi=300, bbox_inches="tight")
    plt.close()
    print("ğŸ“Š ÄÃ£ lÆ°u confusion_matrix.png")
    
    print("\nâœ… Xong! BÃ¢y giá» báº¡n cÃ³ thá»ƒ cháº¡y 'lite_detection_system.py' Ä‘á»ƒ test.")

# %%
if __name__ == "__main__":
    main()
# %%
